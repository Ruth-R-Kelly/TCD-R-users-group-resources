---
title: "glm to MCMCglmm"
author: "Kevin Healy"
date: "24 May 2017"
output: pdf_document
---

This is a short example of linear modeling starting with a glm and working up towards a MCMCglmm. However, we will mainly focus on the basics needed to run and check MCMCglmm models with some extra more complex models including adding random effects.

\section{Installation}

First we need to install some packages including the lme4 to run mixed models using a frequentist approach and MCMCglmm to run a Bayesian approach to mixed models.

```{r install_packages, results="hide", message=FALSE, warning=FALSE}
if(!require(MCMCglmm)) install.packages("MCMCglmm")
if(!require(lme4)) install.packages("lme4")
if(!require(MCMCpack)) install.packages("MCMCpack")
```

We will also install from GitHub the \href{https://github.com/TGuillerme/mulTree}{\texttt{MulTree} package} which is still under development (so watch out for BUGS) but contains some handy data and also will allow us to use MCMCglmm and later to include the error associated with building phylogenies.
To do so we need to get them from GitHub and so we need to run.
```{r install_mulTree, results="hide", message=FALSE}
if(!require(devtools)) install.packages("devtools")
library(devtools)
install_github("TGuillerme/mulTree", ref = "master")
```
If you have not heard of GiTHub it is a great way to share code, build packages and use version control to back up your work. \href{http://rogerdudler.github.io/git-guide/}{For more check out here}

Next we load up the packages we just installed from the library and we are good to go.

```{r load_pakages, results="hide", message=FALSE, warning=FALSE}
library(MCMCglmm)
library(lme4)
library(mulTree)
library(MCMCpack)
```


\section{Data}

For the duration of the tutorials we will use some data that is part of a \href{https://github.com/TGuillerme/mulTree}{\texttt{MulTree} package}. To get it just run

```{r load_data, message=FALSE, warning=FALSE}
data(lifespan)
```

This data file contains a subset of the data used in an analysis on the role of flying (volant) in the evolution of maximum lifespan in birds and mammals \href{http://rspb.royalsocietypublishing.org/content/281/1784/20140298}{Link to paper}. Note that these data have been log transformed, mean centered and expressed in units of standard deviation. The original lifespan data were taken from the \href{http://genomics.senescence.info/species/}{Anage database}. We will come back to why it is often useful to transform data into z-scores later but for now we will simply assume our data is well behaved. Lets have a look at it now.

```{r show_data, message=FALSE, warning=FALSE}
#data have been log transformed, mean centered and 
#expressed in units of standard deviation.
head(lifespan_volant)

```

\section{Lets run some models}

Let's first start off running a simple glm for a subset of data for mammals
```{r setting formula, message=FALSE, warning=FALSE}
#subset for mammals
lifespan_mammals <- lifespan_volant[lifespan_volant$class == "Mammalia",]

#### and run a simple glm
glm_mod <- glm(formula = longevity ~ mass + volant, family = "gaussian", data = lifespan_mammals)
summary(glm_mod)
```

We can plot the results 
```{r plot lifespan increase, message=FALSE, warning=FALSE}
#simple plots
plot(longevity ~ mass, data = lifespan_mammals, pch = 16, bty = "l",
                       xlab = "log10 mass", ylab = "log10 lifespan")
#add in the volant species as red
points(lifespan_mammals[lifespan_mammals$volant == "volant","longevity"] ~ 
        lifespan_mammals[lifespan_mammals$volant == "volant","mass"],
        col = "red", pch = 16)
#add in the nonvolant regression line
abline(glm_mod, lwd = 2)
#add in the volant regression line
abline(glm_mod$coefficients[1] + glm_mod$coefficients[3],
       glm_mod$coefficients[2], lwd = 2, col = "red")
```

Most people will be familiar with lm and glm models so we won't spend any more time here. One thing that we might do is account for some of the structure in the error term. To do so we would need to include a random term.

\subsection{glmm}
While our linear model looks good we might also want to try to control for the structure of our data. For example, maybe all the data points are not fully independent with some data points more likely to have values closer to other ones. In this case we might want to add a random term to control for this. We can imagine such a case in our data above were two species from the same genus might show more similar values of lifespan in comparison to other species. 

Let's plot it out and have a look

```{r checking mixed effect, message=FALSE, warning=FALSE}
#use the gsub function to create a vector of the genus for each species
genus <- (gsub("_.*","", lifespan_mammals$species))

#bind it back into the database
lifespan_mammals <- data.frame(lifespan_mammals, genus = genus)

#plot out lifespan ~ genus to get an idea of whether genus is structured
#randomly
plot(longevity ~  genus, data = lifespan_mammals)
```

Genus is something I would like to control for, however I am not really interested in which groups are different and for practical reasons I don't want to fit every single group as a fixed factor. Hence I could include it as a random term using a lmer model which is used to fit mixed models using a maximum likelihood  approach.

```{r mixed effect, message=FALSE, warning=FALSE}
#Lets fit our model. For lmer the random term is fitted using (1|genus) 
#to indicate that you want to fit seperate intercepts 
#to each of group in your random term.
lmer_mod  <-  lmer(longevity ~ mass + volant  + (1|genus), data = lifespan_mammals)
summary(lmer_mod)
```

Like before we see estimates for each of the fixed effects which look similar to our glm model. The next section that we are interested in is the random effects were we see the terms genus and Residual. Our residual term is acting as in the glm model telling how much of the variance is unexplained while the genus term tells us how much of the variance is due to variance associated with the genus groupings. We can see here for example that genus accounts for more variation than our random term after accounting for our fixed effects. 

I will leave lmer models here, however if you are interested in more on mixed effects models using this approach check out \href{http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf
}{\texttt{this tutorial}}. For now lets move on to the main event with MCMCglmm.

\subsection{MCMCglmm}

So far we have fitted a very simple glm and a glmm model, now we will fit a linear model and a mixed model using the MCMCglmm package. We will start first with a model similar to our glm.

First things first, since we are using a Bayesian approach we will need to set up the priors. In most cases we want to use a non-informative prior that doesn’t influence the estimated posterior distribution. We are basically saying that we don't know anything about the expected values for our parameters. That is we have no prior information of what the intercepts and slopes will be.

To give priors for MCMCglmm we need to make an object that is in a list format that includes terms of B (fixed effect), R (residual terms) and G (random terms which we will come to later).

For now let's build a prior with just a fixed term and a residual term.

```{r priors, message=FALSE, warning=FALSE}

prior <- list(B = list(mu= diag(3)*0, V=diag(3)*1e+10),
            R = list(nu=0.002, V=1))
```

For fixed effects (B) the terms V and mu give the variance and mean of a normal distribution. Here we set mu as 0 and the variance as a large number to make these priors uninformative. Since we have three fixed terms (two intercepts and one slope) we can use the diag function to create a matrix to store a prior for each. Normally we don't need to set this as MCMCglmm will set non-informative priors automatically for fixed terms.

If we plot it out, we can see that higher values for V give less informative priors.

```{r draw some normal curves, message=FALSE, warning=FALSE}
#create some normal distributions over some range x
x <- seq(-10, 10, length=100)
#V = 1
hx_1 <- dnorm(x, 0,1 )
#V = 10
hx_10 <- dnorm(x, 0,10 )
#V = 1e+10
hx_1e10 <- dnorm(x, 0,1e+10 )
plot(x, hx_1, type="l", lty=2, xlab="x value", 
     ylab="Density", main="fixed effects prior")

lines(x, hx_10, lwd=2, col="blue")
lines(x, hx_1e10, lwd=2, col="red")
labels <- c("1","10","1e+10")
legend("topright", inset=.05, title="variance",labels, 
       lwd=2, lty=c(2, 1, 1), col=c("black", "blue", "red"))
```

For the variance terms we need to make sure that the distribution is bounded at zero as the variance term needs to be positive. To do this MCMCglmm uses an \href{https://en.wikipedia.org/wiki/Inverse-gamma_distribution
f}{inverse-Gamma distribution}. In MCMCglmm this is described by two parameters `nu` and `V`. These terms are related to the shape (alpha) and scale (beta) parameters on an inverse-Gamma with `alpha = nu/2`, and `Beta = (nu*V)/2`.
As we don’t want our estimates to be heavily influenced by our prior we will use weakly informative prior values such as descripted as `V = 1` and `nu = 0.002`. (For more on priors the see \href{https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf}{course notes})

```{r draw some gamma inverse curves, message=FALSE, warning=FALSE}
#create some normal distributions over some range x
x <- seq(0, 10, length=10000)
#nu = 0.002 V = 1
hx_0.002 <- dinvgamma(x, 0.002/2, (0.002*1)/2)
#nu = 0.2 V = 1
hx_0.02 <- dinvgamma(x, 0.02/2, (0.02*1)/2)
#nu = 0.002 V = 2
hx_0.002_2 <- dinvgamma(x, 1/2, (1*1)/2)
plot(x, hx_0.002_2, type="l", lty=2, xlab="x value", 
     ylab="Density", main="fixed effects prior")

lines(x, hx_0.02, lwd=2, col="blue")
lines(x, hx_0.002, lwd=2, col="red")
labels <- c("nu = 1","nu = 0.02","nu = 0.002")
legend("topright", inset=.05, title="nu and V",labels, 
       lwd=2, lty=c(2, 1, 1), col=c("black", "blue", "red"))
```


Next we need to decide on the parameters relating to running the mcmc chain in the model. We need to include how many iterations we want to run the MCMC chain for (nitt), the burnin we want to discard at the start of the chain (burnin) and also how often we want to sample and store from the chain (`thin`). We discard a burnin as we don't want the starting point of the chain to over-influence our final estimates. For now let's just use a burnin of 1/6 of the `nitt`, just to be safe. The thinning is used to help reduce autocorrelation in our sample, how much you use often depends on how much autocorrelation you find.

To save time we will only run this model over 12000 iterations (However, much larger `nitt` is often required).

```{r parameters, message=FALSE, warning=FALSE}
#no. of interations
nitt <- c(12000)
#length of burnin
burnin <- c(2000)
#amount of thinning
thin <- c(5)
```


Now we can run the model using our data. Let's run a model similar to our first glm

```{r MCMCglmm_run, message=FALSE, warning=FALSE, verbose = FALSE}

mod_mcmc_fix <- MCMCglmm(fixed =  longevity ~ mass + volant, 
                     family="gaussian",
                     data = lifespan_mammals,
                     nitt = nitt,
                     burnin = burnin,
                     thin = thin,
                     prior = prior)
```

As the model runs we see the iterations print out. These chains can take some time to run, depending on the model, however, since we only ran our chains for 12000 iterations it doesn’t take long here.

Before we even look at our model we need to check if the model ran appropriately. We can do this by visually inspecting the chains to make sure there has been no unruly behavior! We can extract the full chains using `model$Sol` for the fixed effects and `model$VCV` for the variance terms. So `Sol[,1]` will give you the first fixed term, in this case the intercept, and `VCV[,1]` will give you the first random term, which is just the residual term here. As our model is an mcmc object when we use the plot function we get a trace plot.

```{r MCMCglmm_plot, message=FALSE, warning=FALSE, verbose = FALSE}
#plot the fist fixed term, the intercpet.
plot(mod_mcmc_fix$Sol)
#plot the fist variance term, the residual error term.
plot(mod_mcmc_fix$VCV)
```

On the right hand side of the plots is the posterior distributions for each of the terms. On the left side of these plots are the traces of the mcmc chain for each estimate. What we want to see in these trace plots is "hairy caterpillars" (not my phrase!). That is a trace with no obvious trend that is bouncing around some stable point. 

What we don’t want to see in the trace plots can be demonstrated if we only run a model over a very short chain (itt == 10000) or more difficult model fit (we will see these more difficult models later). Notice in the example below that without a burnin the start of trace is well outside the area that the chain is converging towards.

```{r MCMCglmm_crap_run, message=FALSE, warning=FALSE, verbose = FALSE, echo=FALSE}
mod_mcmc_poorfit <- MCMCglmm(fixed = volant ~ mass, 
                                family="catagorical",
                                data = lifespan_mammals,
                                nitt = c(100000),
                                burnin = c(1),
                                thin = c(100),
                                verbose=FALSE)
traceplot(mod_mcmc_poorfit$Sol[,1])
```

So far in our simple mod_mcmc_fix model everything looks good visually, however we also want to check the level of auto-correlation in these traces. We can do this using autocorr.diag() which gives the level of correlation along the chain between some lag sizes.

```{r check auto correlation, message=FALSE, warning=FALSE, verbose = FALSE}
autocorr.diag(mod_mcmc_fix$Sol)
autocorr.diag(mod_mcmc_fix$VCV)
```

or we can look at autocorrelation plots for each of the traces. For example, let's check the auto-correlation in the intercept chain using the `acf` function

```{r acf, message=FALSE, warning=FALSE, verbose = FALSE}
#acf plot for the first fixed estimate in our model (the intercept)
acf(mod_mcmc_fix$Sol[,1], lag.max =100)
```

For our intercept the auto-correlation plot looks good. However if there is bad auto-correlation one quick way to deal with this is to simply increase the thinning. While we don't need to in our case an example would be running something like

```{r long run, message=FALSE, warning=FALSE}
nitt2 <- 240000
burnin2 = 40000
thin2 = 100
mod_mcmc_long <- MCMCglmm(fixed = longevity ~ mass + volant, 
                     family="gaussian",
                     data = lifespan_mammals,
                     nitt = nitt2,
                     burnin = burnin2,
                     thin = thin2,
                     prior = prior,
                     verbose=FALSE)

```

Noticed I also increased the number of iterations. One rough and ready rule that I like to use is to aim for an effective sample size of somewhere between 1000-2000 for all my estimates. The effective sample size is the number of samples in the posterior after the burnin, thinning and autocorrelation are accounted for.

```{r effective sample size, message=FALSE, warning=FALSE, verbose = FALSE}
#acf plot for the first fixed estimate in our model (the intercept)
effectiveSize(mod_mcmc_long$Sol)
effectiveSize(mod_mcmc_long$VCV)
```

*One thing to note is that while thinning might help autocorrelation it wont solve it and you might have to use parameter expanded priors. These are priors that help weight the chain away from zero, a common problem when variance is low or with certain phylogenetic structures. They work by splitting the prior into 2 components with one component weighing the chain away from zero. We will come back to such a prior shortly.

One last thing to check is that our MCMC chain has properly converged and that our estimate is not the result of some type of transitional behaviour. That is have our chains "found" the optimum or do we need to let them run longer before they settle around some estimate. To check this we will run a second model and see if it converges on the same estimates as our first model. 

```{r second mcmc mod, message=FALSE, warning=FALSE, verbose = FALSE}
mod_mcmc_2 <- MCMCglmm(fixed = longevity ~ mass + volant, 
                     family="gaussian",
                     data = lifespan_mammals,
                     nitt = nitt2,
                     burnin = burnin2,
                     thin = thin2,
                     prior = prior,
                     verbose=FALSE)
```

We can now check the convergence of the two chains using the Gelman and Rubin Multiple Sequence Diagnostic. This calculates the within-chain and between-chain variance of the chains and then gives a scale reduced factor, (\href{http://svitsrv25.epfl.ch/R-doc/library/coda/html/gelman.diag.html}{for more see here}. When this number is close to one (something below 1.1 is usually good) the chains are indistinguishable and hence can be considered to be converged.

```{r convergance test, message=FALSE, warning=FALSE, verbose = FALSE}
#checking convergence for our fixed factors
gelman.diag(mcmc.list(mod_mcmc_long$Sol, mod_mcmc_2$Sol))

#checking convergence for our random terms
gelman.diag(mcmc.list(mod_mcmc_long$VCV, mod_mcmc_2$VCV))
```

Since everything looks good, we will finally look at the results of our model.

```{r MCMCglmm_summay, message=FALSE, warning=FALSE, verbose = FALSE}
summary(mod_mcmc_long)
```

First off we can find the estimates for the fixed factors are under the Location effects section (Notice the similarity to our glm and nlme model). Each parameter has a measure of the effect size under post.mean and a lower and higher 95% credible interval (CI). These are simply calculated from the posterior distributions we looked at in the above plots, so if you would rather calculated the median instead of using the mean we can simple use

```{r MCMCglmm_median, message=FALSE, warning=FALSE, verbose = FALSE}
median(mod_mcmc_long$Sol[,1])
```

We also have the effective sample size (`eff.samp`) and the `pMCMC` which calculated as two times the probability that the estimate is either > or <  0, using which ever one is smaller. However, since our data has been mean centred and expressed in units of standard deviation we can simply look at what proportion of our posterior is on either side of zero. This mean centering and expressing our data in units of standard deviation hence allows us to use a cut off point like a p-value but without boiling down the whole distribution to one value.

We also have the DIC which is a Bayesian version of AIC. Like AIC it is a measure of the trade-off between the "fit" of the model and the number of parameters, with a lower number better.

\subsection{Putting the m in MCMCglmm}
Since we can run a simple linear MCMCglmm lets add a random term of genus in the example above. Like before we need to set up the prior however we will let the model estimte the fixed effects this time. To add a random term we now add a `G structure` that acts just like the other random varience terms and is defined using `nu` and `V`.

```{r priors mixed, message=FALSE, warning=FALSE}
prior <- list(G = list(G1 = list(nu=0.002, V=1)),
              R = list(nu=0.002, V=1))
```

We can now include the random term in the model in the section `random= ~`.

```{r mixed effects MCMCglmm, message=FALSE, warning=FALSE, verbose = FALSE}

nitt_m <- 1000
burnin_m <- 1
thin_m <- 1

mod_mcmc_mixed <- MCMCglmm(fixed = longevity ~ mass + volant,
                           rcov=~ units, 
                           random= ~ genus, 
                           family="gaussian",
                           data = lifespan_mammals,
                           nitt = nitt_m,
                           burnin = burnin_m,
                           thin = thin_m,
                           prior = prior,
                           verbose=FALSE)
summary(mod_mcmc_mixed)
```

As an exercise check the output of this model.

Once you are happy with the above model you have essentially got the basics of running a Bayesian mixed effects model. Building on the complexity involves adding more terms which are covered in good detail in the \href{https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf}{course notes} and \href{https://cran.r-project.org/web/packages/MCMCglmm/vignettes/Overview.pdf}{vignette}.


\section{Phylogentic models}

We have already ran a model with genus as a random term in order to attempt to deal with pseudoreplication. However, life is not ordered in catagoires but as a continuous branching tree meaning we are potentially throwing away a lot of useful information. To include the full phylogeny we will use both a Phylogenetic generalized linear model and a MCMCglmm animal model to run the Bayesian version of the phylogenetic comparative analysis. 

First we will need ape and caper, two packages that are very useful for phylogentic analysis.
```{r install_caper_packages, results="hide", message=FALSE, warning=FALSE}
if(!require(ape)) install.packages("ape")
if(!require(caper)) install.packages("caper")
library(ape)
library(caper)
```

We will use a phylogeny of mammals constructed in \href{http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00103.x/abstract}{\texttt{Kuhn et al 2011}}, were they produce 10,000 trees with each individual tree comprising one resolution of the polytomies of a previously published supertree. For now we will just use the first tree in the list.

```{r plot_mammals, message=FALSE, warning=FALSE, fig.width=8,fig.height=10}
# The 10Ktrees from Khun et al (2011) gives a set of trees to represent different polytomies.
# For now let just take one.
mammal_tree <- trees_mammalia[[1]]
plot(mammal_tree, cex = 0.3)
#the number of species
Ntip(mammal_tree)
#we can also check that its ultrametric
is.ultrametric(mammal_tree)
```



\subsection{PGLS}
In phylogenetic comparative methods we try to deal with this non-independence by using the structure of a phylogeny to weight the error term so that our model fit is no longer blind to the non-independence of our data. 

The first step is to make sure that our data and phylogeny match up correctly. We will use the comparative.data function from the caper package to make sure that the species in the dataset are matched up to the species in the phylogeny. To do so we need to give the phylogeny (phy); the dataset and the name of the column in the dataset with the species names (names.col). 
As we want to calculated a variance-covariance matrix of the phylogeny we set vcv = true.
 

```{r comparative.data, message=FALSE, warning=FALSE}

comp_data <- comparative.data(phy = mammal_tree, 
                              data =lifespan_volant, 
                              names.col = species, 
                              vcv=TRUE)

head(comp_data$data)
##notice in the comp_data$data that there are now no birds
###these have been dropped
head(comp_data$dropped)
```

We can now run some models, first lets run two models with lambda fixed to 1 and something close to 0.

```{r fixed lambda, message=FALSE, warning=FALSE}

###lets define our fixed factors
formula_a <- longevity ~ mass + volant

#object comp_data which contains but the phylogeny and the data.
#Lets set the lambda in this case to 1. 

pgls_l1 <- pgls(formula = formula_a, data = comp_data, lambda = c(1))
pgls_l0 <- pgls(formula = formula_a, data = comp_data, lambda = c(0.01))
summary(pgls_l1)
summary(pgls_l0)
```

 Next lets run a model were lambda is no longer fixed. We can do this by specifying lambda = "ML" which tells the model to estimate it using maximum likelihood.

```{r running pgls, message=FALSE, warning=FALSE}

#Finally we also need to set the lambda in this case to ML. 
#This means the we will using Maximum Likelihood
#to calculate the lambda.
pgls_mod <- pgls(formula = formula_a, data = comp_data, lambda = "ML")
summary(pgls_mod)
```

Now under Branch length transformations we also now get the estimated branch transformation under maximum likelihood. As we are only interested in fitting only lambda for now the other types of transformations, (kappa and delta), are held fixed.

Lambda here estimated as 
```{r lambda, message=FALSE, warning=FALSE}
pgls_mod$param["lambda"]
```

As it is close to 1 the traits in this model are correlated under Brownian motion. If our value was 0 it would indicate that our data points are essentially independent.
We can then go a check various elements of the model such as the likelihood profile for lambda.

```{r lambda_profile, message=FALSE, warning=FALSE}
mod_profile <- pgls.profile(pgls_mod)
plot(mod_profile)
```

which looks good as the profile shows a nice clear peak. 


We would also then go ahead and check our residuals etc but for now we will assume everything is good and move onto running a similar model using MCMCglmm


\subsection{MCMCglmm}

So far we have fitted a pgls model that included phylogeny to account for non-independence. 
Now we will use a Bayesian approach were we include phylogeny as a random term using the animal model in the MCMCglmm package.

As we are using a Bayesian approach we will first set up the priors. In most cases we want to use a non-informative prior that doesn’t influence the estimated posterior distribution.
For the random effect  prior we will use an inverse-Gamma distribution. In MCMCglmm this is described by two parameters nu and V. These terms are related to the shape (alpha) and scale (beta) parameters on an inverse-Gamma with alpha = nu/2, and Beta = (nu*V)/2.
As we don’t want our estimates to be heavily influenced by our prior we will use weakly informative prior values such as descripted as V = 1 and nu = 0.002. (For more on priors for the animal model see \href{https://cran.r-project.org/web/packages/MCMCglmm/vignettes/CourseNotes.pdf}{course notes})


```{r priors for animal, message=FALSE, warning=FALSE}
prior <- list(R = list(V=1, nu=0.002), 
              G = list(G1 = list(V=1, nu=0.002)))
```


We describe our prior as above for the random (G) and residual variances (R) each of them as a list, which we will in turn put within a list. If we wanted to include more random terms we would include a G2, G3 etc for each additional random term within the G list. We could also specify priors for the fixed terms using B, however MCMCglmm will automatically do that for us and as it  usually does a good job at it we will ignore it here. 

Next we need to decide on the parameters relating to running the mcmc chain in the model. We need to include how many iterations we want to run the the chain for (nitt), the burnin we want to discard at the start of the chain (burnin) and also how often we want to sample and store from the chain (thin). We discard a burnin as we don't want the starting point of the chain to over-influence our final estimates. For now lets just use a burnin of 1/6 of the nitt, just to be safe. The thinning is used to help reduce autocorrelation in our sample, how much you use often depends on how much autocorrelation you find.

To save time we will only run this model over 12000 iterations (However, much larger nitt is often required).

```{r parameters for animal, message=FALSE, warning=FALSE}
#no. of interations
nitt <- c(12000)
#length of burnin
burnin <- c(2000)
#amount of thinning
thin <- c(5)
```

Now we need to set up the data. We have already cleaned and matched up our data earlier using the comparative.data function but we need to now add an extra column into our dataset called "animal" which contains the species matched between the tree and the data.

```{r MCMCglmm_data for animal, message=FALSE, warning=FALSE}
#Matched data
mcmc_data <- comp_data$data
#As MCMCglmm requires a colume named animal for it to identify it
#as a phylo model we include an extra colume with the species names in it.
mcmc_data <- cbind(animal = rownames(mcmc_data), mcmc_data)
mcmc_tree <- comp_data$phy
```

MCMCglmm reserves the random variable "animal" to call a model that includes the phylogeny as an additive genetic effect. If we name it something else, like say "species", MCMCglmm will either throw an error looking for "animal", or if we do not provide a phylogeny under pedigree it will run "species" like a standard random term.
Now we can run the model.

```{r MCMCglmm animal, message=FALSE, warning=FALSE, verbose = FALSE}

mod_mcmc <- MCMCglmm(fixed = formula_a, 
                     random= ~ animal, 
                     family="gaussian",
                     pedigree = mcmc_tree, 
                     data = mcmc_data,
                     nitt = nitt,
                     burnin = burnin,
                     thin = thin,
                     prior = prior)
```

As the model runs we see the iterations print out. These chains can take some time to run, depending on the model, however, since we only ran our chains for 12000 iterations it doesnt take long here.

Before we even look at our model we need to check if the model ran appropriately. We can do this by visually inspecting the chains to make sure there has been no unruly behaviour! We can extract the full chains using model$Sol for the fixed effects and model$VCV for the random effect variances. So Sol[,1] will give you the first fixed term, in this case the intercept, and VCV[,1] will give you the first random term, which is "animal" and so on. As our model is an mcmc object when we use the plot function we get a trace plot.

```{r MCMCglmm_animal_plot, message=FALSE, warning=FALSE, verbose = FALSE}
plot(mod_mcmc$Sol)
plot(mod_mcmc$VCV)

```

What we want to see "hairy caterpillars" and a trace with no obvious trend that is bouncing around some stable point. 

What we don’t  want to see in the trace plots can be demonstrated if we only run our model over a very short chain (itt == 1000). Notice that without a burnin the start of trace is well outside the area that the chain will converges towards.

```{r MCMCglmm_animal_crap_run, message=FALSE, warning=FALSE, verbose = FALSE, echo=FALSE}
mod_mcmc_short_run <- MCMCglmm(fixed = formula_a, 
                     random= ~ animal, 
                     family="gaussian",
                     pedigree = mcmc_tree, 
                     data = mcmc_data,
                     nitt = c(1000),
                     burnin = c(1),
                     thin = c(1),
                     prior = prior,
                     verbose=FALSE)
traceplot(mod_mcmc_short_run$VCV[,2])
```

So in our longer run model everything looks good visually, however we also want to check the level of autocorrelation in these traces. We can do this using autocorr.diag() which gives the level of correlation along the chain between some lag sizes.

```{r check animal auto correlation, message=FALSE, warning=FALSE, verbose = FALSE}
autocorr.diag(mod_mcmc$Sol)
autocorr.diag(mod_mcmc$VCV)
```

or we can look at autocorrelation plots for each of the traces, we'll look at just one using the acf function here.

```{r acf animal, message=FALSE, warning=FALSE, verbose = FALSE}
#acf plot for the first fixed estimate in our model (the intercept)
acf(mod_mcmc$Sol[,1], lag.max =20)

#acf plot for the first random term in our model (the animal term)
acf(mod_mcmc$VCV[,1], lag.max =20)
```

For our intercept the autocorrelation plot looks good, however the animal term still shows some autocorrelation. One quick way to deal with this is to simply increase the thinning. 

```{r long animal run, message=FALSE, warning=FALSE}

nitt2 <- 240000
burnin2 = 40000
thin2 = 100
mod_mcmc_long <- MCMCglmm(fixed = formula_a, 
                     random= ~ animal, 
                     family="gaussian",
                     pedigree = mcmc_tree, 
                     data = mcmc_data,
                     nitt = nitt2,
                     burnin = burnin2,
                     thin = thin2,
                     prior = prior,
                     verbose=FALSE)

acf(mod_mcmc_long$VCV[,1], lag.max =20)
```

That looks better now. Noticed I also increased the number of iterations. One rough and ready rule that I like to use is to aim for an effective sample size of my chains, which is the number of iterations used in the posterior after the burnin, thinning and accounting for autocorrelation, somewhere between 1000-2000.

```{r animal effective sample size, message=FALSE, warning=FALSE, verbose = FALSE}
#acf plot for the first fixed estimate in our model (the intercept)
effectiveSize(mod_mcmc_long$Sol)
effectiveSize(mod_mcmc_long$VCV)
```



One last thing to check is that our MCMC chain has properly converged and that our estimate is not the result of some type of transitional behaviour. That is have our chains "found" the optimum or do we need to let them run longer before they settle around some estimate. To check this we will run a second model and see if it converges on the same estimates as our first model. 

```{r second animal mcmc mod, message=FALSE, warning=FALSE, verbose = FALSE}
mod_mcmc_2 <- MCMCglmm(fixed = formula_a, 
                     random= ~ animal, 
                     family="gaussian",
                     pedigree = mcmc_tree, 
                     data = mcmc_data,
                     nitt = nitt2,
                     burnin = burnin2,
                     thin = thin2,
                     prior = prior,
                     verbose=FALSE)
```

We can now check the convergence of the two chains using the Gelman and Rubin Multiple Sequence Diagnostic. This calculates the within-chain and between-chain variance of the chains and then gives a scale reduced factor,  (\href{http://svitsrv25.epfl.ch/R-doc/library/coda/html/gelman.diag.html}{for more see here}. When this number is close to one (say below 1.1) the chains are indistinguishable and hence can be considered to be converged.

```{r animal convergance test, message=FALSE, warning=FALSE, verbose = FALSE}
#checking convergence for our fixed factors
gelman.diag(mcmc.list(mod_mcmc_long$Sol, mod_mcmc_2$Sol))

#checking convergence for our random terms
gelman.diag(mcmc.list(mod_mcmc_long$VCV, mod_mcmc_2$VCV))
```

Since everything looks good, we will finally look at the results of our model.

```{r MCMCglmm_animal_summay, message=FALSE, warning=FALSE, verbose = FALSE}
summary(mod_mcmc_long)
```

First off we can find the estimates for the fixed factors are under the Location effects section (Again notice the similarity to our pgls model). Each parameter has a measure of the effect size using the post.mean and a lower and higher 95% credible interval (CI). These are simply calculated from the posterior distributions we looked at in the above plots, so if you would rather calculated the median instead of using the mean we can simple use

```{r MCMCglmm_animal_median, message=FALSE, warning=FALSE, verbose = FALSE}
median(mod_mcmc_long$Sol[,1])
```

We also have the effective sample size (eff.samp) and the pMCMC which calculated as two times the probability that the estimate is either > or <  0, using which ever one is smaller. However since our data has been mean centred and expressed in units of standard deviation we can look at what proportion of our posterior is on either side of zero.

For the random terms we have the posterior distribution for our G-structure which includes or phylogenetic effect and the R-structure which is our residual variation.

We also have the DIC which is a Bayesian version of AIC. Like AIC it is a measure of the trade-off between the "fit" of the model and the number of parameters, with a lower number better.

Finally, we can also calculate the H^2 which is comparable to pagels lambda as

```{r heritability, message=FALSE, warning=FALSE, verbose = FALSE}
H <- (var(mod_mcmc_long$VCV[,"animal"]))/
      (var(mod_mcmc_long$VCV[,"animal"]) 
       + var(mod_mcmc_long$VCV[,"units"]))
H
```


Before moving on to the next section try running the above analysis subsetted for birds as opposed to mammals.

```{r aves tree , message=FALSE, warning=FALSE, fig.width=8,fig.height=10}
#aves tree from Jetz et al 2012
aves_tree <- trees_aves[[1]]
```



